full_transcript: "Okay, hello everybody and welcome to Tuesday, or whatever day it\
  \ is. Today, we're going to do something not that different but with a different\
  \ piece of equipment. This is an eye tracker, so I'm going to put it on my face\
  \ and show you my eyeballs up close. Every time I've come up here, I've tried to\
  \ give a short lecture, but I end up talking for a long time and leaving no time\
  \ at the end. So this time, I'm not going to give much of a lecture at all; I'm\
  \ just going to talk while I'm wearing it, which will take its own kind of time,\
  \ and I'll give a technical preview.\n\nEye tracking data takes longer to process\
  \ than the free MoCap stuff, which I designed intentionally to be easy to pop out\
  \ cool things in a hurry. The idea will be severalfold. One will be to gather data\
  \ on the machine, and then Aaron and I and others can process it over the next week.\
  \ Next Tuesday, we can talk about the data, and at that time, I will discuss ocular\
  \ physiology and the visual aspects of your nervous system.\n\nIt's a kind of flip-floppy\
  \ structure where we will start by jumping into the deep end, and you will have\
  \ all sorts of questions about what I'm doing and various points I mentioned but\
  \ didn\u2019t elaborate on, as well as your own personalized interests in this topic.\
  \ After I finish that, hopefully relatively early, we will split into individualized\
  \ groups. We'll have a couple of moments to drop questions into the lecture chat\
  \ about what you want to know more about. This will allow me to gather that information\
  \ before I plan what I'm going to talk about next week.\n\nWe\u2019re not going\
  \ to spend the entire second half of the class on that, which is why I\u2019m pausing.\
  \ Splitting time is always difficult, but I think it's fine if it transitions softly.\
  \ I will ask you to put some chats into the relevant lecture chat about vision,\
  \ eye tracking, eye movements, and so on. Then we can smoothly transition into a\
  \ conversation more aligned with the AI Scrapy topics I\u2019ve been showing you\
  \ in various forms. Now, I did a little bit of extra polishing, but mostly I realized\
  \ that in Obsidian, when you're viewing the scraped data, you can just search for\
  \ your own member ID on Discord and see all the stuff you've been talking about.\
  \ You'll get a mini graph, and that will be your first opportunity to see your own\
  \ extracted data. I would like you to take a look at that and consider how much\
  \ it represents your interests. Think about how you might include other information\
  \ to make it align more closely with your interests. Also, specifically, consider\
  \ comparing notes with those in your local vicinity. Try to find overlap between\
  \ the topics you've been discussing. For instance, we might both be vaguely interested\
  \ in a general topic, or even better, there could be an empty space between the\
  \ things we've been discussing that could link our particular nodes together.\n\n\
  In the spirit of that, I'm going to quickly show you how to open the data again.\
  \ You'll need Obsidian on your computer and a way to open 7z files. I'm not exactly\
  \ sure what is and isn't built in these days, but Seven Zip is the software we tend\
  \ to use. I prefer zip files because they're more generally compatible, but I created\
  \ these on my Windows partition, so zip files don't work due to the historical limit\
  \ on path lengths.\n\nHere we are in the server. I changed the picture, so you'll\
  \ have to scan for it differently. Up here under resources, there's a section at\
  \ the bottom called server scrape checkpoints. Checkpoints in the software world,\
  \ and specifically regarding machine learning and AI, generally means this is something\
  \ we will keep doing. However, this is a time-stamped checkpoint, which is just\
  \ the terminology there. So click into that, scroll down to the bottom. This is\
  \ more of my notes, just so I can keep track of things, like noting when a bunch\
  \ of stuff popped up on a certain topic due to a lecture, and so on.\n\nClick the\
  \ download button. It will probably download into your downloads folder as dd7z.\
  \ You might be able to double-click it, and it will open automatically. On a Windows\
  \ machine, you may need to right-click it, and on a Mac, I guess you could double-click\
  \ it. Go to Seven Zip and then extract here or something similar. If you don't have\
  \ that option, just Google how to open a 7z file. If that doesn't work, open a chat\
  \ in the general chat and ask about how to open a 7z file on a Mac or whatever.\
  \ The kind of computer you have will help you, and if you can't figure it out, don't\
  \ stress. Someone will ask a neighbor or just chill, and I'll show you later. Once\
  \ you have it on your computer and you've extracted it into a regular old folder,\
  \ the folder will look something like this. I cleaned up some of the pathing; there\
  \ used to be a bunch of sub-paths and stuff like that. Also, there's no raw data\
  \ in here, it's just the AI processed stuff. If you don't remember what that is,\
  \ it does not matter. You are going to need to know this path. If you're not familiar,\
  \ I've probably said this before, but it bears repeating: files on your computer\
  \ are arranged hierarchically. There's like a tree structure where everything is,\
  \ and the first one is like the specific physical drive. Then, there are slashes\
  \ that tell you which branch of the tree you're going down. This thing is called\
  \ a path; it's sometimes also called a directory or a folder, and it just branches\
  \ to tell you how to get to the particular data blob that we call files. In this\
  \ case, I have to go all the way down here. This thing is called a file. MD means\
  \ it's markdown, and its path, location, or full path name is all of these words\
  \ plus the file name plus the extension at the end. The extension at the end is\
  \ just a clue for your computer about how the data will be structured inside it.\
  \ So if the computer tries to open it, it recognizes, \"Oh, it's an MD; I know how\
  \ to handle those.\" I could just as easily have written TXT, and that would be\
  \ raw text. That's about as deep as I feel I need to go down that rabbit hole. So\
  \ in Obsidian, if you followed along last time, you probably opened something similar\
  \ to Manage Vault. I clicked down here and did Manage Vaults; I probably could have\
  \ done Control N or something like that\u2014actually, probably not. It doesn't\
  \ matter. We did the Open Folder as Vault, and then you opened the last one. To\
  \ make things easier, just open a new one labeled \"Open Folder as Vault.\" You\
  \ can see this one is already kind of pre-loaded with the last one I was looking\
  \ at, but this is one of those cases where you can tell that we're not using the\
  \ tool for its intended purpose because it's not behaving in a way that would be\
  \ compatible. There isn't like an easy way\u2014actually, there is an easy way,\
  \ but I'm not going to do it\u2014to tell it to just update the one that exists\
  \ with all these ones. Find the parts that overlap, keep those, and add new content.\
  \ I could probably just... yeah, but it's fine. So here we are: human movement,\
  \ neuroscience, fall 24, checkpoint 24, 10/22, which is today, at 6:39 when I ran\
  \ it. Then, AI process, so it's not the raw. Select the folder, and for some reason,\
  \ it pops back to light mode. Indexing complete pops up, which is kind of interesting\
  \ for deep reasons that I won't get into. Here we are again, and once again, these\
  \ top-level folders\u2014again, Obsidian calls vaults folders; it's just the same\
  \ sort of branding for them. You can more easily see the tree-like hierarchy in\
  \ this format. Macs kind of have this drop-down menu thing in their file system,\
  \ which is nice; Windows does not. They don't give you the extension here because\
  \ it's in software operating under markdown, so it doesn't show that by default.\
  \ The code and AI that I use to generate this is almost exactly the same, except\
  \ that I added a bit of extra stuff to turn these tags into backlinks. If I click\
  \ on this, I'm not sure what will happen. Oh, it might want double brackets... yes,\
  \ it did want double square brackets, so that's fine. If you remember last time,\
  \ I think I talked about how the tags were too disparate; they made one tag for\
  \ virtual reality and another tag for virtual reality questions or something like\
  \ that. It's processing each chat; the unit it's operating under is the chat, cycling\
  \ through each chat object. I just added a bit of extra instructions that said tags\
  \ should be one to two words and don't separate them out into multiple categories.\
  \ With this type of AI programming, often just giving better instructions yields\
  \ better results, similar to how humans work. If I click on this little graph structure\
  \ over here, the graph-looking icon, it pops this out. If I recall correctly, I\
  \ think there were like 222 chats total from all of you, which is nice. I did also,\
  \ now actually, note that this does not include the bot playground chats. That has\
  \ now officially been excluded, even though I thought it was really funny when they\
  \ showed up. You can see that none of them are linked together because there are\
  \ no backlinks. We're not showing tags yet; we're just showing the backlinks. The\
  \ only actual link that we show is this one here, which is the one that I clicked\
  \ on as a demo. So, that\u2019s the one, and then it\u2019s trying to click into\
  \ that one, which is empty. So, there's that. Click on that. Then, this is of course\
  \ the fun part where we say that in the upper right, we have a dropdown for filters\
  \ and then we tell it to show us the tags. These are the tags. I don't have direct\
  \ data to compare from the last time to this time, but just looking at it, it appears\
  \ better; you have fewer of these orphan spots over here, and things are generally\
  \ more connected. Neuromechanics. Yeah, so neuromechanics was a popular topic; I\
  \ think just because the name is so cool. So theoretically, if I click on this,\
  \ it will show every chat that the bot has extracted the tag 'neuromechanics' from.\
  \ If you want to start looking at your space within that, you might need to go into\
  \ user settings and enable Developer Mode. Come on, buddy. Advanced. Turn on Developer\
  \ Mode, which I think is not on by default. Then right-click on your name or double-click\
  \ on your name and go down to copy your user ID. If that doesn't show up, then you\
  \ need to turn on Advanced Mode. Click on that, and it\u2019s now in your clipboard,\
  \ which is just a thing your computer keeps track of for you. Go in here and you\
  \ can search for filters. In the filters box, I could look for something specific\
  \ or look for a different category. I could say, for example, 'only give me stuff\
  \ from the path that includes the chats from whatever category or whatever channel',\
  \ and that would filter at that sort of tree level. I could search by tag, and then\
  \ I think the one that's actually coming up is by line, or just raw paste. Your\
  \ ID is going to be a big number like that. That seems wrong because I haven\u2019\
  t gotten myself in here. So, let's see which unfortunate soul Catherine has been\
  \ discussing these things with. Another thing happening here is that you're supposed\
  \ to play video games or engage with complicated things. It starts out simple and\
  \ then gets complicated. By the time it becomes complicated, you're sort of acclimated\
  \ to it. This is what your graph stuff is supposed to look like when you start,\
  \ and you begin to add notes. Katherine has had one, two, three, four, five chats,\
  \ which, at 222 by 40, is pretty much right on average. So good job there, minus\
  \ any bot playground stuff. I'm not sure why there are no tags showing up here,\
  \ which could just be a problem. I'm not sure why the tags aren't appearing, maybe\
  \ it's because it's a glitch. Anyway, I can go down into the second section called\
  \ groups. I cannot search for two at once for whatever reason because there's nothing\
  \ really explicitly tagging it to you, except that when I made the code to display\
  \ the raw conversations, it happens to show the user ID. I don't know if that would\
  \ technically count as identifying information, but it's certainly not useful for\
  \ most people. Then there is the bot; every chat will have this included. Luckily,\
  \ I happened to add that tag, so you can manually search for your information. This\
  \ way you can get a sense of the landscape of the topics you are interested in.\
  \ However, comparing against your friends and neighbors won\u2019t be super helpful\
  \ just for user interface purposes. In the second tab down here, we can do groups.\
  \ I can assign a color to Katherine's chats, and then view them within the larger\
  \ context. I can add a new group and look at Corina. I made them yellow, but that\
  \ kind of blends in with the green. Let's turn off the tags, and actually, that\
  \ doesn\u2019t really help anymore anyway. So, yellow is a bad color for this case;\
  \ let\u2019s choose, I don't know, blue. And then, this will be a case where you\
  \ could start looking for stuff. This is one of those things where there's an automated\
  \ way to do this that's kind of hard to implement. There's also a simpler, human\
  \ way where you just look at it and click on things, which is easier to do but harder\
  \ to scale. This is a way to approach the topic effectively. So, we'll get back\
  \ into that, but I just wanted to reach this point while I'm talking. That way,\
  \ when the actual second half kicks in, only those who are having trouble with this\
  \ will need instruction. \n\nOkay, questions? Probably. Yeah, great. Cool, we'll\
  \ come back to it. \n\nSo, eye trackers\u2014this is an eye tracker. Well, this\
  \ is a box, but it contains an eye tracker. Several lectures ago, we set up cameras\
  \ that recorded me, and then we extracted data from that. We discussed that data\
  \ in detail. This piece of equipment is similar in many ways; it is a camera-based\
  \ system. The term for this type of system is video oculography. \"Video\" refers\
  \ to the video itself, \"ulo\" pertains to the eyeball, and \"ography\" relates\
  \ to measurement\u2014though, in this case, it's not strict measurement. \n\nBefore\
  \ we proceed, there's another important aspect: the cameras are relatively independent\
  \ of one another. They all go through the same wire to be recorded, but they are\
  \ not rigidly attached. Therefore, there will need to be a calibration process,\
  \ as we demonstrated previously, to make sense of the data. Additionally, working\
  \ with this equipment can be challenging, and I'm not completely convinced it will\
  \ work, so let me clarify that. I know it's not going to work to its fullest extent\
  \ right now because I have already tried that earlier today and it didn't. The problem\
  \ I had was that it wasn't recording from both eyes at the same time; it was having\
  \ some processing issues. But that's okay because, for our purposes here, one eye\
  \ versus two eyes is fine. I have healthy stereo vision; I do not have strabismus,\
  \ which is the technical term for what people call a lazy eye, where your eyes are\
  \ not aligned when you fixate. This means that you can generally use the behavior\
  \ of one eye to tell you about both eyes. If I did have strabismus, I would just\
  \ tell it to point at my dominant eye, which is the one that people who have strabismus\
  \ tend to align with naturally. Everybody has a dominant eye, much like you have\
  \ a dominant hand. Yours is probably your right eye, but it may not be. We'll talk\
  \ about that if there's sufficient interest in the server chats.\n\nThere are many\
  \ eye trackers in this world, and my favorite at the moment is the one made by Pupil\
  \ Labs, a German company that spun off from an academic group. I like them because\
  \ they are very open-source friendly; all their code is open source. I have a lot\
  \ of complaints about the specifics of that code and the software associated with\
  \ it. However, at this point in my life, I have written enough of this software\
  \ that I could not possibly understand more why it is the way it is. You can search\
  \ for Pupil Labs, find their front page, and check out their GitHub page if you're\
  \ curious about what that code looks like. They also have videos and other resources,\
  \ which is nice.\n\nA lot of their newer stuff has a very machine learning bent\
  \ to it; they use machine learning to track the pupil in the eyes, which I really\
  \ don't like. Their old system used classical computer vision techniques, which\
  \ means it is more focused on direct computation of available data with essentially\
  \ no machine learning in the inference step. The process between the empirical measurement\
  \ that happens on the cameras and the resulting data that I want to analyze for\
  \ scientific purposes is rigid. It may not work all the time, but at least I know\
  \ what each step is.\n\nAnytime you have a system that uses machine learning as\
  \ part of its core processing pipeline, there is an inherent stochastic nature to\
  \ the outcome because there is a step that uses machine learning. Machine learning\
  \ algorithms rely on inference rather than direct computation, making their outcomes\
  \ probabilistic. This is a matching type of thing, but that's not important right\
  \ now. Let's see, so capture... It's 2:16 and I want to be done within half an hour.\
  \ Okay, there you go. Yeah, as I thought, we're not going to look at both eyes.\
  \ It's also a little reassuring now that I've written enough specific camera code\
  \ to recognize the problems. They also have it, which means I may not be a total\
  \ dummy. \n\nOkay, so behold this. This is the world camera. It is meant to mimic,\
  \ but is obviously not exactly the same as my viewpoint. We're pretending that this\
  \ view is my eye view; however, we know that that's not the case because my eye\
  \ is here and the camera is here. There's at least a centimeter or so of misalignment\
  \ between this world camera view and my actual eyeball view. \n\nAs much as I want\
  \ to talk about that because I think it's cool, I'm going to leave that for the\
  \ future. Of course, the star of the show... Let's do my left eye. That's my right\
  \ eye. Wait, why is that... Oh, is that why it's wrong? Excuse me, did I just...\
  \ Is this the same one I took off this side? With anyone watching? \n\nOkay, I put\
  \ the other one down and then I was like, wait, is this the same? I don't know.\
  \ We're going to find out. Welcome to life as a scientist with ADHD. It's like,\
  \ did I take notes? No? Do it again? Fine. Luckily, this is why I also don't work\
  \ with expensive equipment. \n\nOkay, so this is wrong. I don't think this will\
  \ make a difference, but it\u2019s like I did this earlier; I swapped the cameras\
  \ out, and I just noticed that the right eye was labeled as I1. I happen to know\
  \ that it should be the other way around. I don\u2019t think there's anything that\
  \ would cause the problems we just saw, but we're going to see. \n\nOkay, this is\
  \ now my left eye. Yeah, all right. And when in doubt, shut it down. There are two\
  \ white Bishops, if you know that. If I said... Have I said that? Before, if you're\
  \ playing chess and you look at the board and both of your bishops are on the white\
  \ squares, there are no legal moves you can make to fix it. So just wipe the board\
  \ and reset it up again. That's why you shut things down. I also noticed today that\
  \ I think I am recording this at 60 FPS, which means I am asking the computer to\
  \ work. There's no reason that this needs to be at 60 FPS. It's still failing. So\
  \ anyways, that's your intro to technical troubleshooting. Now, they don't go on\
  \ the arms right. It was the right way before, but it still doesn't make sense why\
  \ they are backwards. It must be because it was pointing them out when I put them\
  \ on the little arms. Some of you might have been perturbed by the fact that I just\
  \ dropped this unceremoniously, which is fair. But also, this one at this point\
  \ is kind of my demo one. I know enough about this technology to know that it's\
  \ fine. Well, I shouldn't say that on camera. It's fine, not really. But okay, shut\
  \ it down, turn it off, and turn it back on again. That's the classic tech joke.\
  \ It's because of the white bishops thing. And watch it break. That would be really\
  \ funny. So here, I'm shutting it down less powerfully. I do not want to turn off\
  \ the computer, but I will if I have to. Come on, buddy. Wait, what? How was that\
  \ cam ID? Two people? Cam one? I happen to know that one of the things that makes\
  \ this type of stuff hard is device management, like your computer figuring out\
  \ what camera is attached to what. So, I had shut down the software, but I hadn't\
  \ unplugged the cable. You may not like this, but this is actually what most of\
  \ research is like. Come on! Hey, there you go, great! And it even says ID one,\
  \ so there you go. All right, anyway, that's my\u2014uh, so we are recording. This\
  \ is 400 by 400 resolution at 90 FPS, so 90 frames per second, which means that\
  \ every frame that comes off of this thing gives you 90 measurements per second\
  \ off of this camera. And, uh, if it was 100, it would be 10 milliseconds per frame;\
  \ it's 90, so it's a little bit more than that. Uh, no one can do that math. Um,\
  \ yeah, so look at that weird eyeball\u2014look how weird it is. Uh, um, so it's\
  \ a little stretched now because it's 400 by 400, so it should be square, and they\
  \ don't protect the aspect ratio, which they should, but that's fine. So, I will\
  \ actually manually scale it. Like, yeah, one of these things is, if you take a\
  \ square video and you play it through a standard video player, you'll get wrong\
  \ data because it will be skewed, because most of our videos are skewed like that.\
  \ But this is actually just the display, which is fine. And see anything else I\
  \ want to show you? Down here, I think I can do this. I'm not going to try to do\
  \ this on the fly; I can do this next time. I'll show you the 3D stuff, like how\
  \ the IM model actually is built. Basically, it pretends like it's a sphere, and\
  \ I can show you the algorithm. Oh, someday this will show up. Come on, reset window\
  \ size. Oh, that doesn't work. I tell you, guys, this old code. Okay, so that is\
  \ my eyeball. You may recognize it; on average, each of you will have slightly less\
  \ than two of these, and they will work roughly similar to that. There's a number\
  \ of interesting parts. You can see that kind of like this is my contact lens. This\
  \ part here is the iris, easily one of the top ten human sphincters. Irises are\
  \ basically a hole that will sort of contract in response to things such as light,\
  \ but also to things like emotional arousal and cognitive load and stuff like that.\
  \ There is some research called pupilometry that basically looks at eye trackers\
  \ and examines entirely the behavior of your pupil and the size and shape of your\
  \ pupil, and how it changes on different tasks. I'm going to talk some right now.\
  \ The majority of research involving eye tracking is pupilometry because the majority\
  \ of scientists are lazy cowards. It's not that it's not an interesting signal to\
  \ look at, but it is not as interesting as the volume of research would suggest.\
  \ So, why do we look at it? Because it doesn't require you to properly calibrate\
  \ your equipment. It doesn't require you to think about things in the world and\
  \ sort of feel feedback loops and stuff like that. So, when you look for research\
  \ on eye tracking, the majority of it is going to be pupilometry. I'm not saying\
  \ don't look at that, but I am saying... I'm not saying anything. So, you will also\
  \ see... Yeah, so you've got the iris, good old iris. You\u2019ve got the eyelid,\
  \ an important guy. The eyelid\u2019s job is... Mainly to keep it moist, blinking\
  \ is one of those behaviors that we don't think is interesting, but actually there\
  \ is a lot that goes into when we should blink. In your normal everyday life, you\
  \ blink when your eyes are dry. I don't actually know what the normal cadence is,\
  \ but you blink when your eyes are dry, and that\u2019s no big deal. However, when\
  \ you start doing really difficult visual tasks\u2014like landing a plane or walking\
  \ over rocky terrain, for example\u2014in my research, you blink far less often.\
  \ The cost of being blind for 50 milliseconds goes up when doing a really difficult\
  \ task. You'll see this interesting behavior when people perform hard tasks. In\
  \ my research on people walking over rocky terrain, they would stand at the front\
  \ and blink multiple times: blink, blink, blink, blink. Then, as they walked, they\
  \ would virtually stop blinking. When they reached the end, they would go back to\
  \ blinking: blink, blink, blink, blink, blink. The only times you really saw blinks\
  \ were when they were looking from the ground up to the goal and then back. This\
  \ amount of time is like dead time, so people\u2019s visual nervous systems\u2014\
  not consciously, but somehow\u2014sort of know that that's a good time to blink.\
  \ There\u2019s a scheduling mechanism and some relevant neurons called omnipause\
  \ neurons that handle that. \n\nSo, class is over at 1:25. Great, doing great. Here\
  \ we have an eyeball and a world. We believe that these two things are connected\
  \ in some way. What we want to be able to see is something that tells you where\
  \ in this image I am looking based off the data available from this image. I have\
  \ no idea why this is not showing the overlay, but, again, I understand. \n\nLet\
  \ me real quick just dump a little more information. This is the 'drink from the\
  \ fire hose' phase where I just say a bunch of stuff and then you try to notice\
  \ which parts of it were interesting. You\u2019ll notice that the eye moves in strange\
  \ ways; sometimes it makes jerky movements called saccades. I'm looking here, and\
  \ I'm going to look there\u2014finger to finger. Those jerky little eye movements\
  \ are called saccades. The term 'saccade' is French for jerk because they are jerky\
  \ movements. It is actually the fastest movement that your body can make. Oh, and\
  \ also, those two little lights there are infrared lights. Emitters are IEDs, infrared\
  \ emitting diodes, which are like LEDs but for infrared. You can see the reflection\
  \ there. If I cover them up, my eye gets very dark. You can actually see my finger\
  \ reflected there. The thing that I find even more interesting is that you see that\
  \ kind of ghostly eyeball. When you look at the eyeball, the first thing you see\
  \ is actually the contact lens. Behind the contact lens, you can see the cornea,\
  \ which is the clear part on the outside. If you don't do this right now, later,\
  \ pick a friend and try to look at their eye from the side. You'll see a little\
  \ bulbous, clear dome; that's your cornea. The majority of the bending of light,\
  \ which must happen for your image to be in focus, takes place at the cornea. When\
  \ you wear something like a contact lens, you're changing the refractive index to\
  \ make things line up because your eyes are not shaped perfectly. The shape of your\
  \ eyes is somewhat unique. Past the cornea, the pupil is just a void; it's just\
  \ an empty hole. Behind that, you have the lens. The lens has muscles attached to\
  \ it, and it stretches. If you look at something close by and manage to make your\
  \ eyes go out of focus, that activity is about changing the shape of your lens to\
  \ override the automated focusing systems occurring at various stages of your nervous\
  \ system. You might wonder, how do we know how to focus? How do we know to focus?\
  \ When I\u2019m looking close, I should make my lens one shape, but when I\u2019\
  m looking far, I should make my lens a different shape. Somewhere in the goop, there\
  \ is a part that functions similarly to the autofocus on a camera, although it is\
  \ very different in every other aspect. After that, it travels back through the\
  \ lens, then into the vitreous humor, which is this clear goo. Then you reach the\
  \ retina, which is strangely backward because when we crawled out of the ocean,\
  \ a lot of things inverted, including our eyes. Our retinas are backward. Next,\
  \ you encounter the pigment epithelium, which is the dark layer at the back. If\
  \ you have albinism, you struggle with melanin production, and as a result, you\
  \ may have vision problems. Finally, you reach the back of your eye, which is the\
  \ sclera\u2014 the white part of the eye\u2014 and there you find the optic nerves\
  \ and all that good stuff. So, if you notice... Those two white glowy B spots are\
  \ hiccuping again. Oops, that's the first reflection of the infrared. The infrared\
  \ IEDs\u2014like the brightest part is the part where it hits the front of the cornea.\
  \ I probably should mention very briefly next time Snell's Law: when light hits\
  \ a surface, it can do one of three things. It can reflect, which is to bounce off;\
  \ it can be absorbed; or it can refract, which means to change direction. Someone\
  \ nodded, which suggests that perhaps they've taken a class on this. I learned it\
  \ recently, and I only very recently learned that this phenomenon is called Snell's\
  \ Law, so I was kind of guessing\u2014nailed it! \n\nYou get these sort of ghostly\
  \ reflections; some of which move in phase, and some move out of phase. Those are\
  \ going to be some version of the first, second, third, and fourth order images,\
  \ which are basically the reflections as the light moves through the different tissues\
  \ of the eye. From the cornea to the front of the lens to the back of the lens,\
  \ every time it hits a surface, the reflective part bounces back out. Because the\
  \ camera is so close, we can see where that\u2019s happening, which is interesting.\
  \ Some very high-end eye trackers use those secondary, tertiary, and quaternary\
  \ images to achieve very high accuracy in eye tracking, but we don't do that here.\
  \ \n\nOkay, so we're hiccuping again. Another thing to note about the software:\
  \ I happen to know that the people who write this code work on Linux. This is a\
  \ Windows machine, so generally, if you code in MATLAB on Linux, it works well on\
  \ Linux and Macintosh. However, Windows is a very different system, which leads\
  \ to the errors we see. \n\nAll right, let\u2019s get close to finishing. What I'm\
  \ going to do now is try to calibrate the eye tracker so that we can get some real-time\
  \ measurement of where I'm looking. It\u2019s not going to be as good as it could\
  \ be; however, I\u2019ll do some post-processing to improve the calibration afterward.\
  \ I'm going to hit C, and I think if I do, it will pop up. I look at the indicator,\
  \ and come on! Buddy, I\u2019m looking at the Bull's Eye. There is insufficient\
  \ pupil data. It may not happen because of this. Okay, all right, so I\u2019m going\
  \ to record some of this data later. We start with default settings, but I will\
  \ show you the kinds of stuff. Oh, there it goes. That\u2019s me; it\u2019s actually\
  \ tracking my eye. [Music] In 490 annual mode, that\u2019s probably good. Okay,\
  \ so eye movements there. Sometimes you see these really jerky eye movements; those\
  \ are called saccades. Those are usually voluntary. Sometimes you also see these\
  \ smooth eye movements. I\u2019m fixating on the camera, moving my head, and my\
  \ eyes are moving nice and smooth. Those are sometimes called smooth eye movements.\
  \ More specifically, those eye movements are the result of the vestibular ocular\
  \ reflex, which is one of the lowest level and oldest reflexes. We have evidence\
  \ dating back to bony fish, which is about 450 million years ago. It is basically\
  \ a very short feedback loop, connecting the vestibular system, which measures head\
  \ acceleration and rotation. That counter-rotates the eye relative to my head movement\
  \ so that I can fixate on objects in the world, even as I'm moving around, and the\
  \ image on my retina remains clear. The chemical involved in vision is based on\
  \ opsins, which are strange chemicals that are photo-biochemical and electrically\
  \ active. When they absorb a photon, they change shape, which alters their electrical\
  \ properties. This change sets off a neural cascade, resulting in vision. However,\
  \ opsins are relatively slow, taking about 10 to 15 milliseconds to respond to light,\
  \ which is slow enough to cause problems. For instance, if you fixate on your thumb\
  \ and move your head around, the blur in the background is a result of the world\
  \ moving too fast for your opsins to keep up. To manage this, we have various gaze\
  \ stabilization mechanisms that are fundamental to our nervous system. They allow\
  \ us to pick objects we are focusing on and keep them stabilized on the back of\
  \ our retina, giving our opsins time to react to the light and send back clear images.\
  \ During quick eye movements, we are essentially blind because the world moves too\
  \ quickly. There are complex reasons for why we can't see during rapid movements,\
  \ but when we fixate on something and move our heads, the stabilization occurs through\
  \ mechanisms like the vestibular ocular reflex and optokinetic nystagmus. Another\
  \ type of eye movement is called smooth pursuit eye movements. With a healthy human\
  \ nervous system, I cannot move my eyes smoothly across the world. If I try to move\
  \ my eyes in a smooth line along the back of the room, it will appear jerky because\
  \ I will be making small saccades. However, I can perform smooth movements if I\
  \ am fixating on a target. These are known as smooth pursuit eye movements, which\
  \ are a relatively recent addition in terms of evolution. They are peculiar because\
  \ they depend on the act of choosing to look at something, allowing for a smooth\
  \ pursuit of that target. Smooth eye movements are interesting. If I'm in a completely\
  \ pitch-dark room where I can't see my hand, I can still make a smooth pursuit eye\
  \ movement tracking my thumb, but only my thumb. Another personal favorite is torsional\
  \ eye movement. It's a torsional eye movement, right? You have a three-dimensional\
  \ object called an eye. It can move left, right, up, and down, but it can also rotate\
  \ around its central axis. You can see as I rotate my head, I am stabilizing it\
  \ in that direction, but not that much. You can see that I am trying to keep up,\
  \ which is always fun.\n\nNow, let's see, what am I doing? I'm looking at the eye.\
  \ I don't know, saccades are weird because we have voluntary control over many parts\
  \ of our bodies, but not all of them. You can control your breathing, but you can't\
  \ control your heart rate. Generally, you don't consciously control your breathing;\
  \ it's under partial voluntary control. Our eye movements are like that. We can\
  \ control them, but we typically don't. They simply do their thing, and if we want\
  \ to pay attention to them and clamp down on them with cognitive voluntary control,\
  \ we can, but it requires effort. What that effort entails is hard to know.\n\n\
  Okay, at 12:45, I'm going to go ahead and record the data, even though I don't think\
  \ it's going to work. So, I'll record this and some other things later, and I'll\
  \ bring it back in next time. We are now recording from both the eye and the world\
  \ camera. They are time-synchronized, so the frames will line up in time, but they're\
  \ not spatially calibrated yet. I'll move that a little bit down and rotate it.\
  \ I don't like their automated calibration system, so I'm going to do a separate\
  \ calibration. This will involve looking at the nail of my thumb. I will be looking\
  \ at that while rotating it, giving myself a reference point in the image that I\
  \ know I am looking at based on the instructions I gave myself. I can use that to\
  \ calibrate it after the fact, and then importantly, I will not keep my thumb in\
  \ the screen when I'm not doing that task because I won't be able to tell otherwise.\
  \ I'm also calibrating. Close. I'm also going to look at the recording symbol over\
  \ there and calibrate, sort of doing that same kind of movement. The calibration\
  \ is somewhat distance dependent, so giving myself a short target and a long target\
  \ will be helpful in that way. So, let's test that it\u2019s actually working. I'm\
  \ looking over there\u2014I don't know from the direct data which one I am looking\
  \ at, but I know that I\u2019m looking at one of those. I can cheat and look at\
  \ the computer screen; that will help me identify what I see there. Ask yourself\
  \ what you think the data will look like if we could see the track of my eye. What\
  \ does that look like in the data trace? What does this look like in the data trace?\
  \ This is a visual motor task, and I\u2019m going to see if I can close my eyes\
  \ and do it. I can\u2019t. Now, let\u2019s see\u2014oh yeah, I\u2019m going to try\
  \ to trace something in the back, and we know that I can\u2019t. Now I\u2019m going\
  \ to try to trace this, and we know that I can. Was there any way to tell from my\
  \ eyeballs the difference between this and this? Maybe not. Just for fun, I\u2019\
  ll look at the screen. Classic calibration. Okay, that\u2019s roughly three minutes\
  \ of recorded data, which is more than enough to spend careers analyzing because\
  \ it\u2019s just so complicated. Anything else? Oh, here I am walking, and I\u2019\
  m going to turn around. Oh, you can\u2019t actually see my eye. I\u2019m not going\
  \ to recalibrate it because I\u2019m going to choose the same calibration, which\
  \ is safe enough. This is something I noticed when I was doing my walking studies:\
  \ you could tell just from the eye data when the person turned around. You see this\
  \ kind of tick, tick, tick, tick because my head is rotating and the VR is trying\
  \ to adjust, but it gets to a limit on the side, so it sort of hits the edge and\
  \ ticks back. What is happening is that it happens like tick, tick, tick. Is that\
  \ okay? Cool. The nervous system works. VR is such a low-level reflex that it is\
  \ used as a proxy for brain death in emergency situations. If you are looking at\
  \ someone and you don't know if they are alive, you can rotate their head. If their\
  \ eyes don't counter-rotate, that's it. That is not a reflex that can fail while\
  \ the rest of the body is working. There you go. Great, another minute of the data,\
  \ a whole of the career, and yeah. Okay, take these off. Luckily, we can't see infrared.\
  \ There are a lot of ways we use the fact that we can't see infrared, specifically\
  \ in human movement studies, but in many cases, because if these were visible light,\
  \ I couldn't use them, as that would be blinding. So instead, they are infrared,\
  \ which is both a low energy wavelength, so it is hopefully not doing too much tissue\
  \ damage, but also you can basically blast my eyes with light and then just have\
  \ a camera that records in that wavelength and sort of make your life work that\
  \ way. Okay, great. I hope that was sufficiently enticing and confusing. I think\
  \ we can call it there. Please dump your questions into the machine and see how\
  \ well it does. I'll be really interested because this is now getting closer to\
  \ my actual proper area of literal expertise. I will be really interested to see\
  \ how the bot does as you ask it questions. It will probably get it totally good\
  \ at the level that you need to worry about, especially because we are not giving\
  \ you tests or anything like that. But I'm really curious about the nuances. Sometimes\
  \ when you ask it specifics, it will probably give you really good best guesses.\
  \ But I'm guessing that there are going to be places where it actually misaligns\
  \ with what I know about how the field is going, my personal beliefs, which are\
  \ things about the nervous system that I cannot prove and are not written down anywhere.\
  \ Other experts may disagree with me, so I will be really interested to see, first\
  \ of all, what you all are interested in, and secondly, how a bot that has been\
  \ trained by consuming the internet does when you get to those limitations on the\
  \ specifics of knowledge. As I have said before, I think it tends to nail anything\
  \ at the level of textbook information, and then it starts to fuzz out as you get\
  \ into the parts of the conversation that have less of a footprint on the data set.\
  \ That this bot ate. Okay, that's not bad, a whole half an hour. All right, bye."
title: HMN24 - 05 - Intro to Eye Tracking
transcript_chunks:
- dur: 180.0
  end: 180.0
  start: 0.0
  text: "Okay, hello everybody and welcome to Tuesday, or whatever day it is. Today,\
    \ we're going to do something not that different but with a different piece of\
    \ equipment. This is an eye tracker, so I'm going to put it on my face and show\
    \ you my eyeballs up close. Every time I've come up here, I've tried to give a\
    \ short lecture, but I end up talking for a long time and leaving no time at the\
    \ end. So this time, I'm not going to give much of a lecture at all; I'm just\
    \ going to talk while I'm wearing it, which will take its own kind of time, and\
    \ I'll give a technical preview.\n\nEye tracking data takes longer to process\
    \ than the free MoCap stuff, which I designed intentionally to be easy to pop\
    \ out cool things in a hurry. The idea will be severalfold. One will be to gather\
    \ data on the machine, and then Aaron and I and others can process it over the\
    \ next week. Next Tuesday, we can talk about the data, and at that time, I will\
    \ discuss ocular physiology and the visual aspects of your nervous system.\n\n\
    It's a kind of flip-floppy structure where we will start by jumping into the deep\
    \ end, and you will have all sorts of questions about what I'm doing and various\
    \ points I mentioned but didn\u2019t elaborate on, as well as your own personalized\
    \ interests in this topic. After I finish that, hopefully relatively early, we\
    \ will split into individualized groups. We'll have a couple of moments to drop\
    \ questions into the lecture chat about what you want to know more about. This\
    \ will allow me to gather that information before I plan what I'm going to talk\
    \ about next week.\n\nWe\u2019re not going to spend the entire second half of\
    \ the class on that, which is why I\u2019m pausing. Splitting time is always difficult,\
    \ but I think it's fine if it transitions softly. I will ask you to put some chats\
    \ into the relevant lecture chat about vision, eye tracking, eye movements, and\
    \ so on. Then we can smoothly transition into a conversation more aligned with\
    \ the AI Scrapy topics I\u2019ve been showing you in various forms."
- dur: 180.0
  end: 360.0
  start: 180.0
  text: 'Now, I did a little bit of extra polishing, but mostly I realized that in
    Obsidian, when you''re viewing the scraped data, you can just search for your
    own member ID on Discord and see all the stuff you''ve been talking about. You''ll
    get a mini graph, and that will be your first opportunity to see your own extracted
    data. I would like you to take a look at that and consider how much it represents
    your interests. Think about how you might include other information to make it
    align more closely with your interests. Also, specifically, consider comparing
    notes with those in your local vicinity. Try to find overlap between the topics
    you''ve been discussing. For instance, we might both be vaguely interested in
    a general topic, or even better, there could be an empty space between the things
    we''ve been discussing that could link our particular nodes together.


    In the spirit of that, I''m going to quickly show you how to open the data again.
    You''ll need Obsidian on your computer and a way to open 7z files. I''m not exactly
    sure what is and isn''t built in these days, but Seven Zip is the software we
    tend to use. I prefer zip files because they''re more generally compatible, but
    I created these on my Windows partition, so zip files don''t work due to the historical
    limit on path lengths.


    Here we are in the server. I changed the picture, so you''ll have to scan for
    it differently. Up here under resources, there''s a section at the bottom called
    server scrape checkpoints. Checkpoints in the software world, and specifically
    regarding machine learning and AI, generally means this is something we will keep
    doing. However, this is a time-stamped checkpoint, which is just the terminology
    there. So click into that, scroll down to the bottom. This is more of my notes,
    just so I can keep track of things, like noting when a bunch of stuff popped up
    on a certain topic due to a lecture, and so on.


    Click the download button. It will probably download into your downloads folder
    as dd7z. You might be able to double-click it, and it will open automatically.
    On a Windows machine, you may need to right-click it, and on a Mac, I guess you
    could double-click it. Go to Seven Zip and then extract here or something similar.
    If you don''t have that option, just Google how to open a 7z file. If that doesn''t
    work, open a chat in the general chat and ask about how to open a 7z file on a
    Mac or whatever.'
- dur: 180.0
  end: 540.0
  start: 360.0
  text: "The kind of computer you have will help you, and if you can't figure it out,\
    \ don't stress. Someone will ask a neighbor or just chill, and I'll show you later.\
    \ Once you have it on your computer and you've extracted it into a regular old\
    \ folder, the folder will look something like this. I cleaned up some of the pathing;\
    \ there used to be a bunch of sub-paths and stuff like that. Also, there's no\
    \ raw data in here, it's just the AI processed stuff. If you don't remember what\
    \ that is, it does not matter. You are going to need to know this path. If you're\
    \ not familiar, I've probably said this before, but it bears repeating: files\
    \ on your computer are arranged hierarchically. There's like a tree structure\
    \ where everything is, and the first one is like the specific physical drive.\
    \ Then, there are slashes that tell you which branch of the tree you're going\
    \ down. This thing is called a path; it's sometimes also called a directory or\
    \ a folder, and it just branches to tell you how to get to the particular data\
    \ blob that we call files. In this case, I have to go all the way down here. This\
    \ thing is called a file. MD means it's markdown, and its path, location, or full\
    \ path name is all of these words plus the file name plus the extension at the\
    \ end. The extension at the end is just a clue for your computer about how the\
    \ data will be structured inside it. So if the computer tries to open it, it recognizes,\
    \ \"Oh, it's an MD; I know how to handle those.\" I could just as easily have\
    \ written TXT, and that would be raw text. That's about as deep as I feel I need\
    \ to go down that rabbit hole. So in Obsidian, if you followed along last time,\
    \ you probably opened something similar to Manage Vault. I clicked down here and\
    \ did Manage Vaults; I probably could have done Control N or something like that\u2014\
    actually, probably not. It doesn't matter. We did the Open Folder as Vault, and\
    \ then you opened the last one. To make things easier, just open a new one labeled\
    \ \"Open Folder as Vault.\" You can see this one is already kind of pre-loaded\
    \ with the last one I was looking at, but this is one of those cases where you\
    \ can tell that we're not using the tool for its intended purpose because it's\
    \ not behaving in a way that would be compatible. There isn't like an easy way\u2014\
    actually, there is an easy way, but I'm not going to do it\u2014to tell it to\
    \ just update the one that exists with all these ones."
- dur: 180.0
  end: 720.0
  start: 540.0
  text: "Find the parts that overlap, keep those, and add new content. I could probably\
    \ just... yeah, but it's fine. So here we are: human movement, neuroscience, fall\
    \ 24, checkpoint 24, 10/22, which is today, at 6:39 when I ran it. Then, AI process,\
    \ so it's not the raw. Select the folder, and for some reason, it pops back to\
    \ light mode. Indexing complete pops up, which is kind of interesting for deep\
    \ reasons that I won't get into. Here we are again, and once again, these top-level\
    \ folders\u2014again, Obsidian calls vaults folders; it's just the same sort of\
    \ branding for them. You can more easily see the tree-like hierarchy in this format.\
    \ Macs kind of have this drop-down menu thing in their file system, which is nice;\
    \ Windows does not. They don't give you the extension here because it's in software\
    \ operating under markdown, so it doesn't show that by default. The code and AI\
    \ that I use to generate this is almost exactly the same, except that I added\
    \ a bit of extra stuff to turn these tags into backlinks. If I click on this,\
    \ I'm not sure what will happen. Oh, it might want double brackets... yes, it\
    \ did want double square brackets, so that's fine. If you remember last time,\
    \ I think I talked about how the tags were too disparate; they made one tag for\
    \ virtual reality and another tag for virtual reality questions or something like\
    \ that. It's processing each chat; the unit it's operating under is the chat,\
    \ cycling through each chat object. I just added a bit of extra instructions that\
    \ said tags should be one to two words and don't separate them out into multiple\
    \ categories. With this type of AI programming, often just giving better instructions\
    \ yields better results, similar to how humans work. If I click on this little\
    \ graph structure over here, the graph-looking icon, it pops this out. If I recall\
    \ correctly, I think there were like 222 chats total from all of you, which is\
    \ nice. I did also, now actually, note that this does not include the bot playground\
    \ chats. That has now officially been excluded, even though I thought it was really\
    \ funny when they showed up."
- dur: 180.0
  end: 900.0
  start: 720.0
  text: "You can see that none of them are linked together because there are no backlinks.\
    \ We're not showing tags yet; we're just showing the backlinks. The only actual\
    \ link that we show is this one here, which is the one that I clicked on as a\
    \ demo. So, that\u2019s the one, and then it\u2019s trying to click into that\
    \ one, which is empty. So, there's that. Click on that. Then, this is of course\
    \ the fun part where we say that in the upper right, we have a dropdown for filters\
    \ and then we tell it to show us the tags. These are the tags. I don't have direct\
    \ data to compare from the last time to this time, but just looking at it, it\
    \ appears better; you have fewer of these orphan spots over here, and things are\
    \ generally more connected. Neuromechanics. Yeah, so neuromechanics was a popular\
    \ topic; I think just because the name is so cool. So theoretically, if I click\
    \ on this, it will show every chat that the bot has extracted the tag 'neuromechanics'\
    \ from. If you want to start looking at your space within that, you might need\
    \ to go into user settings and enable Developer Mode. Come on, buddy. Advanced.\
    \ Turn on Developer Mode, which I think is not on by default. Then right-click\
    \ on your name or double-click on your name and go down to copy your user ID.\
    \ If that doesn't show up, then you need to turn on Advanced Mode. Click on that,\
    \ and it\u2019s now in your clipboard, which is just a thing your computer keeps\
    \ track of for you. Go in here and you can search for filters. In the filters\
    \ box, I could look for something specific or look for a different category. I\
    \ could say, for example, 'only give me stuff from the path that includes the\
    \ chats from whatever category or whatever channel', and that would filter at\
    \ that sort of tree level. I could search by tag, and then I think the one that's\
    \ actually coming up is by line, or just raw paste. Your ID is going to be a big\
    \ number like that. That seems wrong because I haven\u2019t gotten myself in here.\
    \ So, let's see which unfortunate soul Catherine has been discussing these things\
    \ with."
- dur: 180.0
  end: 1080.0
  start: 900.0
  text: "Another thing happening here is that you're supposed to play video games\
    \ or engage with complicated things. It starts out simple and then gets complicated.\
    \ By the time it becomes complicated, you're sort of acclimated to it. This is\
    \ what your graph stuff is supposed to look like when you start, and you begin\
    \ to add notes. Katherine has had one, two, three, four, five chats, which, at\
    \ 222 by 40, is pretty much right on average. So good job there, minus any bot\
    \ playground stuff. I'm not sure why there are no tags showing up here, which\
    \ could just be a problem. I'm not sure why the tags aren't appearing, maybe it's\
    \ because it's a glitch. Anyway, I can go down into the second section called\
    \ groups. I cannot search for two at once for whatever reason because there's\
    \ nothing really explicitly tagging it to you, except that when I made the code\
    \ to display the raw conversations, it happens to show the user ID. I don't know\
    \ if that would technically count as identifying information, but it's certainly\
    \ not useful for most people. Then there is the bot; every chat will have this\
    \ included. Luckily, I happened to add that tag, so you can manually search for\
    \ your information. This way you can get a sense of the landscape of the topics\
    \ you are interested in. However, comparing against your friends and neighbors\
    \ won\u2019t be super helpful just for user interface purposes. In the second\
    \ tab down here, we can do groups. I can assign a color to Katherine's chats,\
    \ and then view them within the larger context. I can add a new group and look\
    \ at Corina. I made them yellow, but that kind of blends in with the green. Let's\
    \ turn off the tags, and actually, that doesn\u2019t really help anymore anyway.\
    \ So, yellow is a bad color for this case; let\u2019s choose, I don't know, blue."
- dur: 180.0
  end: 1260.0
  start: 1080.0
  text: "And then, this will be a case where you could start looking for stuff. This\
    \ is one of those things where there's an automated way to do this that's kind\
    \ of hard to implement. There's also a simpler, human way where you just look\
    \ at it and click on things, which is easier to do but harder to scale. This is\
    \ a way to approach the topic effectively. So, we'll get back into that, but I\
    \ just wanted to reach this point while I'm talking. That way, when the actual\
    \ second half kicks in, only those who are having trouble with this will need\
    \ instruction. \n\nOkay, questions? Probably. Yeah, great. Cool, we'll come back\
    \ to it. \n\nSo, eye trackers\u2014this is an eye tracker. Well, this is a box,\
    \ but it contains an eye tracker. Several lectures ago, we set up cameras that\
    \ recorded me, and then we extracted data from that. We discussed that data in\
    \ detail. This piece of equipment is similar in many ways; it is a camera-based\
    \ system. The term for this type of system is video oculography. \"Video\" refers\
    \ to the video itself, \"ulo\" pertains to the eyeball, and \"ography\" relates\
    \ to measurement\u2014though, in this case, it's not strict measurement. \n\n\
    Before we proceed, there's another important aspect: the cameras are relatively\
    \ independent of one another. They all go through the same wire to be recorded,\
    \ but they are not rigidly attached. Therefore, there will need to be a calibration\
    \ process, as we demonstrated previously, to make sense of the data. Additionally,\
    \ working with this equipment can be challenging, and I'm not completely convinced\
    \ it will work, so let me clarify that."
- dur: 180.0
  end: 1440.0
  start: 1260.0
  text: 'I know it''s not going to work to its fullest extent right now because I
    have already tried that earlier today and it didn''t. The problem I had was that
    it wasn''t recording from both eyes at the same time; it was having some processing
    issues. But that''s okay because, for our purposes here, one eye versus two eyes
    is fine. I have healthy stereo vision; I do not have strabismus, which is the
    technical term for what people call a lazy eye, where your eyes are not aligned
    when you fixate. This means that you can generally use the behavior of one eye
    to tell you about both eyes. If I did have strabismus, I would just tell it to
    point at my dominant eye, which is the one that people who have strabismus tend
    to align with naturally. Everybody has a dominant eye, much like you have a dominant
    hand. Yours is probably your right eye, but it may not be. We''ll talk about that
    if there''s sufficient interest in the server chats.


    There are many eye trackers in this world, and my favorite at the moment is the
    one made by Pupil Labs, a German company that spun off from an academic group.
    I like them because they are very open-source friendly; all their code is open
    source. I have a lot of complaints about the specifics of that code and the software
    associated with it. However, at this point in my life, I have written enough of
    this software that I could not possibly understand more why it is the way it is.
    You can search for Pupil Labs, find their front page, and check out their GitHub
    page if you''re curious about what that code looks like. They also have videos
    and other resources, which is nice.


    A lot of their newer stuff has a very machine learning bent to it; they use machine
    learning to track the pupil in the eyes, which I really don''t like. Their old
    system used classical computer vision techniques, which means it is more focused
    on direct computation of available data with essentially no machine learning in
    the inference step. The process between the empirical measurement that happens
    on the cameras and the resulting data that I want to analyze for scientific purposes
    is rigid. It may not work all the time, but at least I know what each step is.


    Anytime you have a system that uses machine learning as part of its core processing
    pipeline, there is an inherent stochastic nature to the outcome because there
    is a step that uses machine learning. Machine learning algorithms rely on inference
    rather than direct computation, making their outcomes probabilistic.'
- dur: 180.0
  end: 1620.0
  start: 1440.0
  text: "This is a matching type of thing, but that's not important right now. Let's\
    \ see, so capture... It's 2:16 and I want to be done within half an hour. Okay,\
    \ there you go. Yeah, as I thought, we're not going to look at both eyes. It's\
    \ also a little reassuring now that I've written enough specific camera code to\
    \ recognize the problems. They also have it, which means I may not be a total\
    \ dummy. \n\nOkay, so behold this. This is the world camera. It is meant to mimic,\
    \ but is obviously not exactly the same as my viewpoint. We're pretending that\
    \ this view is my eye view; however, we know that that's not the case because\
    \ my eye is here and the camera is here. There's at least a centimeter or so of\
    \ misalignment between this world camera view and my actual eyeball view. \n\n\
    As much as I want to talk about that because I think it's cool, I'm going to leave\
    \ that for the future. Of course, the star of the show... Let's do my left eye.\
    \ That's my right eye. Wait, why is that... Oh, is that why it's wrong? Excuse\
    \ me, did I just... Is this the same one I took off this side? With anyone watching?\
    \ \n\nOkay, I put the other one down and then I was like, wait, is this the same?\
    \ I don't know. We're going to find out. Welcome to life as a scientist with ADHD.\
    \ It's like, did I take notes? No? Do it again? Fine. Luckily, this is why I also\
    \ don't work with expensive equipment. \n\nOkay, so this is wrong. I don't think\
    \ this will make a difference, but it\u2019s like I did this earlier; I swapped\
    \ the cameras out, and I just noticed that the right eye was labeled as I1. I\
    \ happen to know that it should be the other way around. I don\u2019t think there's\
    \ anything that would cause the problems we just saw, but we're going to see.\
    \ \n\nOkay, this is now my left eye. Yeah, all right. And when in doubt, shut\
    \ it down. There are two white Bishops, if you know that. If I said... Have I\
    \ said that?"
- dur: 180.0
  end: 1800.0
  start: 1620.0
  text: Before, if you're playing chess and you look at the board and both of your
    bishops are on the white squares, there are no legal moves you can make to fix
    it. So just wipe the board and reset it up again. That's why you shut things down.
    I also noticed today that I think I am recording this at 60 FPS, which means I
    am asking the computer to work. There's no reason that this needs to be at 60
    FPS. It's still failing. So anyways, that's your intro to technical troubleshooting.
    Now, they don't go on the arms right. It was the right way before, but it still
    doesn't make sense why they are backwards. It must be because it was pointing
    them out when I put them on the little arms. Some of you might have been perturbed
    by the fact that I just dropped this unceremoniously, which is fair. But also,
    this one at this point is kind of my demo one. I know enough about this technology
    to know that it's fine. Well, I shouldn't say that on camera. It's fine, not really.
    But okay, shut it down, turn it off, and turn it back on again. That's the classic
    tech joke. It's because of the white bishops thing. And watch it break. That would
    be really funny. So here, I'm shutting it down less powerfully. I do not want
    to turn off the computer, but I will if I have to.
- dur: 180.0
  end: 1980.0
  start: 1800.0
  text: "Come on, buddy. Wait, what? How was that cam ID? Two people? Cam one? I happen\
    \ to know that one of the things that makes this type of stuff hard is device\
    \ management, like your computer figuring out what camera is attached to what.\
    \ So, I had shut down the software, but I hadn't unplugged the cable. You may\
    \ not like this, but this is actually what most of research is like. Come on!\
    \ Hey, there you go, great! And it even says ID one, so there you go. All right,\
    \ anyway, that's my\u2014uh, so we are recording. This is 400 by 400 resolution\
    \ at 90 FPS, so 90 frames per second, which means that every frame that comes\
    \ off of this thing gives you 90 measurements per second off of this camera. And,\
    \ uh, if it was 100, it would be 10 milliseconds per frame; it's 90, so it's a\
    \ little bit more than that. Uh, no one can do that math. Um, yeah, so look at\
    \ that weird eyeball\u2014look how weird it is. Uh, um, so it's a little stretched\
    \ now because it's 400 by 400, so it should be square, and they don't protect\
    \ the aspect ratio, which they should, but that's fine. So, I will actually manually\
    \ scale it. Like, yeah, one of these things is, if you take a square video and\
    \ you play it through a standard video player, you'll get wrong data because it\
    \ will be skewed, because most of our videos are skewed like that. But this is\
    \ actually just the display, which is fine. And see anything else I want to show\
    \ you?"
- dur: 180.0
  end: 2160.0
  start: 1980.0
  text: "Down here, I think I can do this. I'm not going to try to do this on the\
    \ fly; I can do this next time. I'll show you the 3D stuff, like how the IM model\
    \ actually is built. Basically, it pretends like it's a sphere, and I can show\
    \ you the algorithm. Oh, someday this will show up. Come on, reset window size.\
    \ Oh, that doesn't work. I tell you, guys, this old code. Okay, so that is my\
    \ eyeball. You may recognize it; on average, each of you will have slightly less\
    \ than two of these, and they will work roughly similar to that. There's a number\
    \ of interesting parts. You can see that kind of like this is my contact lens.\
    \ This part here is the iris, easily one of the top ten human sphincters. Irises\
    \ are basically a hole that will sort of contract in response to things such as\
    \ light, but also to things like emotional arousal and cognitive load and stuff\
    \ like that. There is some research called pupilometry that basically looks at\
    \ eye trackers and examines entirely the behavior of your pupil and the size and\
    \ shape of your pupil, and how it changes on different tasks. I'm going to talk\
    \ some right now. The majority of research involving eye tracking is pupilometry\
    \ because the majority of scientists are lazy cowards. It's not that it's not\
    \ an interesting signal to look at, but it is not as interesting as the volume\
    \ of research would suggest. So, why do we look at it? Because it doesn't require\
    \ you to properly calibrate your equipment. It doesn't require you to think about\
    \ things in the world and sort of feel feedback loops and stuff like that. So,\
    \ when you look for research on eye tracking, the majority of it is going to be\
    \ pupilometry. I'm not saying don't look at that, but I am saying... I'm not saying\
    \ anything. So, you will also see... Yeah, so you've got the iris, good old iris.\
    \ You\u2019ve got the eyelid, an important guy. The eyelid\u2019s job is..."
- dur: 180.0
  end: 2340.0
  start: 2160.0
  text: "Mainly to keep it moist, blinking is one of those behaviors that we don't\
    \ think is interesting, but actually there is a lot that goes into when we should\
    \ blink. In your normal everyday life, you blink when your eyes are dry. I don't\
    \ actually know what the normal cadence is, but you blink when your eyes are dry,\
    \ and that\u2019s no big deal. However, when you start doing really difficult\
    \ visual tasks\u2014like landing a plane or walking over rocky terrain, for example\u2014\
    in my research, you blink far less often. The cost of being blind for 50 milliseconds\
    \ goes up when doing a really difficult task. You'll see this interesting behavior\
    \ when people perform hard tasks. In my research on people walking over rocky\
    \ terrain, they would stand at the front and blink multiple times: blink, blink,\
    \ blink, blink. Then, as they walked, they would virtually stop blinking. When\
    \ they reached the end, they would go back to blinking: blink, blink, blink, blink,\
    \ blink. The only times you really saw blinks were when they were looking from\
    \ the ground up to the goal and then back. This amount of time is like dead time,\
    \ so people\u2019s visual nervous systems\u2014not consciously, but somehow\u2014\
    sort of know that that's a good time to blink. There\u2019s a scheduling mechanism\
    \ and some relevant neurons called omnipause neurons that handle that. \n\nSo,\
    \ class is over at 1:25. Great, doing great. Here we have an eyeball and a world.\
    \ We believe that these two things are connected in some way. What we want to\
    \ be able to see is something that tells you where in this image I am looking\
    \ based off the data available from this image. I have no idea why this is not\
    \ showing the overlay, but, again, I understand. \n\nLet me real quick just dump\
    \ a little more information. This is the 'drink from the fire hose' phase where\
    \ I just say a bunch of stuff and then you try to notice which parts of it were\
    \ interesting. You\u2019ll notice that the eye moves in strange ways; sometimes\
    \ it makes jerky movements called saccades. I'm looking here, and I'm going to\
    \ look there\u2014finger to finger. Those jerky little eye movements are called\
    \ saccades. The term 'saccade' is French for jerk because they are jerky movements.\
    \ It is actually the fastest movement that your body can make. Oh, and also, those\
    \ two little lights there are infrared lights."
- dur: 180.0
  end: 2520.0
  start: 2340.0
  text: "Emitters are IEDs, infrared emitting diodes, which are like LEDs but for\
    \ infrared. You can see the reflection there. If I cover them up, my eye gets\
    \ very dark. You can actually see my finger reflected there. The thing that I\
    \ find even more interesting is that you see that kind of ghostly eyeball. When\
    \ you look at the eyeball, the first thing you see is actually the contact lens.\
    \ Behind the contact lens, you can see the cornea, which is the clear part on\
    \ the outside. If you don't do this right now, later, pick a friend and try to\
    \ look at their eye from the side. You'll see a little bulbous, clear dome; that's\
    \ your cornea. The majority of the bending of light, which must happen for your\
    \ image to be in focus, takes place at the cornea. When you wear something like\
    \ a contact lens, you're changing the refractive index to make things line up\
    \ because your eyes are not shaped perfectly. The shape of your eyes is somewhat\
    \ unique. Past the cornea, the pupil is just a void; it's just an empty hole.\
    \ Behind that, you have the lens. The lens has muscles attached to it, and it\
    \ stretches. If you look at something close by and manage to make your eyes go\
    \ out of focus, that activity is about changing the shape of your lens to override\
    \ the automated focusing systems occurring at various stages of your nervous system.\
    \ You might wonder, how do we know how to focus? How do we know to focus? When\
    \ I\u2019m looking close, I should make my lens one shape, but when I\u2019m looking\
    \ far, I should make my lens a different shape. Somewhere in the goop, there is\
    \ a part that functions similarly to the autofocus on a camera, although it is\
    \ very different in every other aspect. After that, it travels back through the\
    \ lens, then into the vitreous humor, which is this clear goo. Then you reach\
    \ the retina, which is strangely backward because when we crawled out of the ocean,\
    \ a lot of things inverted, including our eyes. Our retinas are backward. Next,\
    \ you encounter the pigment epithelium, which is the dark layer at the back. If\
    \ you have albinism, you struggle with melanin production, and as a result, you\
    \ may have vision problems. Finally, you reach the back of your eye, which is\
    \ the sclera\u2014 the white part of the eye\u2014 and there you find the optic\
    \ nerves and all that good stuff. So, if you notice..."
- dur: 180.0
  end: 2700.0
  start: 2520.0
  text: "Those two white glowy B spots are hiccuping again. Oops, that's the first\
    \ reflection of the infrared. The infrared IEDs\u2014like the brightest part is\
    \ the part where it hits the front of the cornea. I probably should mention very\
    \ briefly next time Snell's Law: when light hits a surface, it can do one of three\
    \ things. It can reflect, which is to bounce off; it can be absorbed; or it can\
    \ refract, which means to change direction. Someone nodded, which suggests that\
    \ perhaps they've taken a class on this. I learned it recently, and I only very\
    \ recently learned that this phenomenon is called Snell's Law, so I was kind of\
    \ guessing\u2014nailed it! \n\nYou get these sort of ghostly reflections; some\
    \ of which move in phase, and some move out of phase. Those are going to be some\
    \ version of the first, second, third, and fourth order images, which are basically\
    \ the reflections as the light moves through the different tissues of the eye.\
    \ From the cornea to the front of the lens to the back of the lens, every time\
    \ it hits a surface, the reflective part bounces back out. Because the camera\
    \ is so close, we can see where that\u2019s happening, which is interesting. Some\
    \ very high-end eye trackers use those secondary, tertiary, and quaternary images\
    \ to achieve very high accuracy in eye tracking, but we don't do that here. \n\
    \nOkay, so we're hiccuping again. Another thing to note about the software: I\
    \ happen to know that the people who write this code work on Linux. This is a\
    \ Windows machine, so generally, if you code in MATLAB on Linux, it works well\
    \ on Linux and Macintosh. However, Windows is a very different system, which leads\
    \ to the errors we see. \n\nAll right, let\u2019s get close to finishing. What\
    \ I'm going to do now is try to calibrate the eye tracker so that we can get some\
    \ real-time measurement of where I'm looking. It\u2019s not going to be as good\
    \ as it could be; however, I\u2019ll do some post-processing to improve the calibration\
    \ afterward. I'm going to hit C, and I think if I do, it will pop up. I look at\
    \ the indicator, and come on!"
- dur: 180.0
  end: 2880.0
  start: 2700.0
  text: "Buddy, I\u2019m looking at the Bull's Eye. There is insufficient pupil data.\
    \ It may not happen because of this. Okay, all right, so I\u2019m going to record\
    \ some of this data later. We start with default settings, but I will show you\
    \ the kinds of stuff. Oh, there it goes. That\u2019s me; it\u2019s actually tracking\
    \ my eye. [Music] In 490 annual mode, that\u2019s probably good. Okay, so eye\
    \ movements there. Sometimes you see these really jerky eye movements; those are\
    \ called saccades. Those are usually voluntary. Sometimes you also see these smooth\
    \ eye movements. I\u2019m fixating on the camera, moving my head, and my eyes\
    \ are moving nice and smooth. Those are sometimes called smooth eye movements.\
    \ More specifically, those eye movements are the result of the vestibular ocular\
    \ reflex, which is one of the lowest level and oldest reflexes. We have evidence\
    \ dating back to bony fish, which is about 450 million years ago. It is basically\
    \ a very short feedback loop, connecting the vestibular system, which measures\
    \ head acceleration and rotation. That counter-rotates the eye relative to my\
    \ head movement so that I can fixate on objects in the world, even as I'm moving\
    \ around, and the image on my retina remains clear."
- dur: 180.0
  end: 3060.0
  start: 2880.0
  text: The chemical involved in vision is based on opsins, which are strange chemicals
    that are photo-biochemical and electrically active. When they absorb a photon,
    they change shape, which alters their electrical properties. This change sets
    off a neural cascade, resulting in vision. However, opsins are relatively slow,
    taking about 10 to 15 milliseconds to respond to light, which is slow enough to
    cause problems. For instance, if you fixate on your thumb and move your head around,
    the blur in the background is a result of the world moving too fast for your opsins
    to keep up. To manage this, we have various gaze stabilization mechanisms that
    are fundamental to our nervous system. They allow us to pick objects we are focusing
    on and keep them stabilized on the back of our retina, giving our opsins time
    to react to the light and send back clear images. During quick eye movements,
    we are essentially blind because the world moves too quickly. There are complex
    reasons for why we can't see during rapid movements, but when we fixate on something
    and move our heads, the stabilization occurs through mechanisms like the vestibular
    ocular reflex and optokinetic nystagmus. Another type of eye movement is called
    smooth pursuit eye movements. With a healthy human nervous system, I cannot move
    my eyes smoothly across the world. If I try to move my eyes in a smooth line along
    the back of the room, it will appear jerky because I will be making small saccades.
    However, I can perform smooth movements if I am fixating on a target. These are
    known as smooth pursuit eye movements, which are a relatively recent addition
    in terms of evolution. They are peculiar because they depend on the act of choosing
    to look at something, allowing for a smooth pursuit of that target.
- dur: 180.0
  end: 3240.0
  start: 3060.0
  text: 'Smooth eye movements are interesting. If I''m in a completely pitch-dark
    room where I can''t see my hand, I can still make a smooth pursuit eye movement
    tracking my thumb, but only my thumb. Another personal favorite is torsional eye
    movement. It''s a torsional eye movement, right? You have a three-dimensional
    object called an eye. It can move left, right, up, and down, but it can also rotate
    around its central axis. You can see as I rotate my head, I am stabilizing it
    in that direction, but not that much. You can see that I am trying to keep up,
    which is always fun.


    Now, let''s see, what am I doing? I''m looking at the eye. I don''t know, saccades
    are weird because we have voluntary control over many parts of our bodies, but
    not all of them. You can control your breathing, but you can''t control your heart
    rate. Generally, you don''t consciously control your breathing; it''s under partial
    voluntary control. Our eye movements are like that. We can control them, but we
    typically don''t. They simply do their thing, and if we want to pay attention
    to them and clamp down on them with cognitive voluntary control, we can, but it
    requires effort. What that effort entails is hard to know.


    Okay, at 12:45, I''m going to go ahead and record the data, even though I don''t
    think it''s going to work. So, I''ll record this and some other things later,
    and I''ll bring it back in next time. We are now recording from both the eye and
    the world camera. They are time-synchronized, so the frames will line up in time,
    but they''re not spatially calibrated yet. I''ll move that a little bit down and
    rotate it. I don''t like their automated calibration system, so I''m going to
    do a separate calibration. This will involve looking at the nail of my thumb.
    I will be looking at that while rotating it, giving myself a reference point in
    the image that I know I am looking at based on the instructions I gave myself.
    I can use that to calibrate it after the fact, and then importantly, I will not
    keep my thumb in the screen when I''m not doing that task because I won''t be
    able to tell otherwise. I''m also calibrating.'
- dur: 180.0
  end: 3420.0
  start: 3240.0
  text: "Close. I'm also going to look at the recording symbol over there and calibrate,\
    \ sort of doing that same kind of movement. The calibration is somewhat distance\
    \ dependent, so giving myself a short target and a long target will be helpful\
    \ in that way. So, let's test that it\u2019s actually working. I'm looking over\
    \ there\u2014I don't know from the direct data which one I am looking at, but\
    \ I know that I\u2019m looking at one of those. I can cheat and look at the computer\
    \ screen; that will help me identify what I see there. Ask yourself what you think\
    \ the data will look like if we could see the track of my eye. What does that\
    \ look like in the data trace? What does this look like in the data trace? This\
    \ is a visual motor task, and I\u2019m going to see if I can close my eyes and\
    \ do it. I can\u2019t. Now, let\u2019s see\u2014oh yeah, I\u2019m going to try\
    \ to trace something in the back, and we know that I can\u2019t. Now I\u2019m\
    \ going to try to trace this, and we know that I can. Was there any way to tell\
    \ from my eyeballs the difference between this and this? Maybe not. Just for fun,\
    \ I\u2019ll look at the screen. Classic calibration. Okay, that\u2019s roughly\
    \ three minutes of recorded data, which is more than enough to spend careers analyzing\
    \ because it\u2019s just so complicated. Anything else? Oh, here I am walking,\
    \ and I\u2019m going to turn around. Oh, you can\u2019t actually see my eye. I\u2019\
    m not going to recalibrate it because I\u2019m going to choose the same calibration,\
    \ which is safe enough. This is something I noticed when I was doing my walking\
    \ studies: you could tell just from the eye data when the person turned around.\
    \ You see this kind of tick, tick, tick, tick because my head is rotating and\
    \ the VR is trying to adjust, but it gets to a limit on the side, so it sort of\
    \ hits the edge and ticks back."
- dur: 180.0
  end: 3600.0
  start: 3420.0
  text: What is happening is that it happens like tick, tick, tick. Is that okay?
    Cool. The nervous system works. VR is such a low-level reflex that it is used
    as a proxy for brain death in emergency situations. If you are looking at someone
    and you don't know if they are alive, you can rotate their head. If their eyes
    don't counter-rotate, that's it. That is not a reflex that can fail while the
    rest of the body is working. There you go. Great, another minute of the data,
    a whole of the career, and yeah. Okay, take these off. Luckily, we can't see infrared.
    There are a lot of ways we use the fact that we can't see infrared, specifically
    in human movement studies, but in many cases, because if these were visible light,
    I couldn't use them, as that would be blinding. So instead, they are infrared,
    which is both a low energy wavelength, so it is hopefully not doing too much tissue
    damage, but also you can basically blast my eyes with light and then just have
    a camera that records in that wavelength and sort of make your life work that
    way. Okay, great. I hope that was sufficiently enticing and confusing. I think
    we can call it there. Please dump your questions into the machine and see how
    well it does. I'll be really interested because this is now getting closer to
    my actual proper area of literal expertise. I will be really interested to see
    how the bot does as you ask it questions. It will probably get it totally good
    at the level that you need to worry about, especially because we are not giving
    you tests or anything like that. But I'm really curious about the nuances. Sometimes
    when you ask it specifics, it will probably give you really good best guesses.
    But I'm guessing that there are going to be places where it actually misaligns
    with what I know about how the field is going, my personal beliefs, which are
    things about the nervous system that I cannot prove and are not written down anywhere.
    Other experts may disagree with me, so I will be really interested to see, first
    of all, what you all are interested in, and secondly, how a bot that has been
    trained by consuming the internet does when you get to those limitations on the
    specifics of knowledge. As I have said before, I think it tends to nail anything
    at the level of textbook information, and then it starts to fuzz out as you get
    into the parts of the conversation that have less of a footprint on the data set.
- dur: 180.0
  end: 3780.0
  start: 3600.0
  text: That this bot ate. Okay, that's not bad, a whole half an hour. All right,
    bye.
video_id: TQuf8NJgkkA
